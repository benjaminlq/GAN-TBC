{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/QUAN/Desktop/gan/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Union, Literal\n",
    "from pathlib import Path\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = Path(\"../\")\n",
    "DATA_PATH = MAIN_PATH / \"data\"\n",
    "ARTIFACT_PATH = MAIN_PATH / \"artifacts\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "EPS = 1e-8\n",
    "CLIP = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding = 4),\n",
    "        transforms.ToTensor(),\n",
    "      #  transforms.Normalize(0.5, 0.5)\n",
    "        ])\n",
    "train_dataset = datasets.MNIST(DATA_PATH, download = True, train = True, transform = train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_idx = {digit: [] for digit in range(10)}\n",
    "for idx, (_, label) in enumerate(train_dataset):\n",
    "    mnist_idx[label].append(idx)\n",
    "\n",
    "import json\n",
    "with open(\"./deploy/jsons/mnist.json\", \"w\") as f:\n",
    "    json.dump(mnist_idx, f)\n",
    "    \n",
    "with open(\"./deploy/jsons/mnist.json\", \"r\") as f:\n",
    "    mnist_idx1 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding = 4),\n",
    "        transforms.ToTensor(),\n",
    "      #  transforms.Normalize(0.5, 0.5)\n",
    "        ])\n",
    "fmnist_dataset = datasets.FashionMNIST(DATA_PATH, download = True, train = True, transform = train_transform)\n",
    "fmnist_idx = {class_no: [] for class_no in range(10)}\n",
    "for idx, (_, label) in enumerate(fmnist_dataset):\n",
    "    fmnist_idx[label].append(idx)\n",
    "\n",
    "import json\n",
    "with open(\"./deploy/jsons/fmnist.json\", \"w\") as f:\n",
    "    json.dump(fmnist_idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding = 4),\n",
    "        transforms.ToTensor(),\n",
    "      #  transforms.Normalize(0.5, 0.5)\n",
    "        ])\n",
    "cifar_dataset = datasets.CIFAR10(DATA_PATH, download = True, train = True, transform = train_transform)\n",
    "cifar_idx = {class_no: [] for class_no in range(10)}\n",
    "for idx, (_, label) in enumerate(cifar_dataset):\n",
    "    cifar_idx[label].append(idx)\n",
    "\n",
    "import json\n",
    "with open(\"./deploy/jsons/cifar10.json\", \"w\") as f:\n",
    "    json.dump(cifar_idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Dataloader():\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: Union[str, Path] = DATA_PATH,\n",
    "        batch_size: int = 32,\n",
    "        std_normalize: bool = False,\n",
    "        ):\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        if std_normalize:\n",
    "            train_transform = transforms.Compose([\n",
    "                    transforms.RandomCrop(28, padding = 4),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=0.5, std=0.5)\n",
    "                ])\n",
    "            test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                transforms.Normalize(mean=0.5, std=0.5)\n",
    "                                                ])\n",
    "        else:\n",
    "            train_transform = transforms.Compose([\n",
    "                    transforms.RandomCrop(28, padding = 4),\n",
    "                    transforms.ToTensor(),\n",
    "                ])\n",
    "            test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                ])\n",
    "\n",
    "        self.train_dataset = datasets.MNIST(data_path, download = True, train = True, transform = train_transform)\n",
    "        self.test_dataset = datasets.MNIST(data_path, download = True, train = False, transform = test_transform)\n",
    "        \n",
    "    def train_loader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size = self.batch_size, shuffle=True, drop_last=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    \n",
    "    def test_loader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size = self.batch_size, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int = 784,\n",
    "        common_size: int = 400,\n",
    "        hidden_size: int = 128,\n",
    "        activation: Literal[\"Tanh\",\"Sigmoid\"] = \"Tanh\",\n",
    "    ):\n",
    "\n",
    "        super(BaseVAE, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.common_size = common_size\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.encoder = nn.Linear(input_size, common_size)\n",
    "        self.mean_fc = nn.Linear(common_size, hidden_size)\n",
    "        self.var_fc = nn.Linear(common_size, hidden_size)\n",
    "        self.decoder = nn.Sequential(nn.Linear(hidden_size, common_size), nn.ReLU(),\n",
    "                                     nn.Linear(common_size, input_size))\n",
    "        # self.decode_fc = nn.Linear(hidden_size, common_size)\n",
    "        # self.out_fc = nn.Linear(common_size, input_size)\n",
    "        self.activation = getattr(nn, activation)()\n",
    "        \n",
    "    def encode(self, flattened_inputs: torch.tensor) -> torch.tensor:\n",
    "\n",
    "        x = F.relu(self.encoder(flattened_inputs))\n",
    "        mu = self.mean_fc(x)\n",
    "        log_var = self.var_fc(x)\n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu: torch.tensor, log_var: torch.tensor) -> torch.tensor:\n",
    "\n",
    "        std = torch.exp(0.5*log_var) # Standard deviation of estimated P(z|x)\n",
    "        eps = torch.randn_like(std) # Sample on z ~ N(0,1)\n",
    "        z = mu + eps * std # Reparameterize z to on mu & std\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z: torch.tensor) -> torch.tensor:\n",
    "\n",
    "        out = self.activation(self.decoder(z))\n",
    "        return out\n",
    "    \n",
    "    def forward(self, inputs: torch.tensor):\n",
    "\n",
    "        x = self.flatten(inputs)\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        out = self.decode(z)\n",
    "        return out, mu, log_var\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"VAE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stack(nn.Module):\n",
    "    def __init__(self, channels, height, width):\n",
    "        super(Stack, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), self.channels, self.height, self.width)\n",
    "\n",
    "class DeepVAE(BaseVAE):\n",
    "    def __init__(\n",
    "        self, input_size: int = 784, common_size: int = 128, hidden_size: int = 128, alpha: float = 1.0,\n",
    "        activation: Literal[\"Tanh\",\"Sigmoid\"] = \"Tanh\",\n",
    "    ):\n",
    "        super(DeepVAE, self).__init__(input_size, common_size, hidden_size, activation)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 392), nn.BatchNorm1d(392), nn.LeakyReLU(0.1),\n",
    "            nn.Linear(392, 196), nn.BatchNorm1d(196), nn.LeakyReLU(0.1),\n",
    "            nn.Linear(196, 128), nn.BatchNorm1d(128), nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128, hidden_size)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128), nn.BatchNorm1d(128), nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128, 196), nn.BatchNorm1d(196), nn.LeakyReLU(0.1),\n",
    "            nn.Linear(196, 392), nn.BatchNorm1d(392), nn.LeakyReLU(0.1),\n",
    "            nn.Linear(392, 784),\n",
    "        )\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"DeepVAE\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Custom Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCE_VAE_loss(x_recon: torch.tensor, x: torch.tensor, mu: torch.tensor, log_var: torch.tensor, alpha=1):\n",
    "    \"\"\"VAE Loss using BCE for reconstruction loss\n",
    "\n",
    "    Args:\n",
    "        x_recon (torch.tensor): _description_\n",
    "        x (torch.tensor): _description_\n",
    "        mu (torch.tensor): _description_\n",
    "        log_var (torch.tensor): _description_\n",
    "        alpha (int, optional): _description_. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    BCE = F.binary_cross_entropy(x_recon, x.view(x.size(0), -1), reduction=\"none\").sum(dim=1).mean(dim=0)\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1).mean(dim=0)\n",
    "    return alpha*BCE + KLD, BCE, KLD\n",
    "\n",
    "def MSE_VAE_loss(x_recon: torch.tensor, x: torch.tensor, mu: torch.tensor, log_var: torch.tensor, alpha=1):\n",
    "    \"\"\"VAE Loss using MSE for reconstruction loss\n",
    "\n",
    "    Args:\n",
    "        x_recon (torch.tensor): _description_\n",
    "        x (torch.tensor): _description_\n",
    "        mu (torch.tensor): _description_\n",
    "        log_var (torch.tensor): _description_\n",
    "        alpha (int, optional): _description_. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    MSE = nn.MSELoss()(x_recon, x.view(x.size(0), -1))\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1).mean(dim=0)\n",
    "    return alpha*MSE + KLD, MSE, KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path: str):\n",
    "    \"\"\"Save Model\n",
    "\n",
    "    Args:\n",
    "        model (Callable): Model\n",
    "        path (str): Path to save ckpt\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model {str(model)} saved successfully at {path}\")\n",
    "\n",
    "\n",
    "def load_model(model, path: str):\n",
    "    \"\"\"Load Model\n",
    "\n",
    "    Args:\n",
    "        model (Callable): Model\n",
    "        path (str): Path to load ckpt\n",
    "    \"\"\"\n",
    "    model.load_state_dict(torch.load(path, map_location=DEVICE))\n",
    "    print(f\"Model {str(model)} loaded successfully from {path}\")\n",
    "    \n",
    "def plot_images(imgs):\n",
    "    \"\"\"Plot Images\n",
    "\n",
    "    Args:\n",
    "        imgs (Sequence): Sequence of images\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=len(imgs), squeeze=True)\n",
    "    for i in range(len(imgs)):\n",
    "        img = imgs[i].detach()\n",
    "        img = torchvision.transforms.functional.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    plt.show()\n",
    "    \n",
    "def seed_everything(seed: int = 2023):\n",
    "    \"\"\"Set seed for reproducability\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def plot_images(imgs):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=len(imgs), squeeze=True)\n",
    "    for i in range(len(imgs)):\n",
    "        img = imgs[i].detach()\n",
    "        img = torchvision.transforms.functional.to_pil_image(img)\n",
    "        axs[i].imshow(np.asarray(img))\n",
    "        axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    plt.show()\n",
    "    \n",
    "def compare_recon(original_images, recon_images):\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=len(original_images), squeeze=True)\n",
    "    for i in range(len(original_images)):\n",
    "        origin_img = original_images[i].detach()\n",
    "        recon_img = recon_images[i].detach()\n",
    "        origin_img = torchvision.transforms.functional.to_pil_image(origin_img)\n",
    "        recon_img = torchvision.transforms.functional.to_pil_image(recon_img)\n",
    "        axs[0, i].imshow(np.asarray(origin_img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "        axs[1, i].imshow(np.asarray(recon_img))\n",
    "        axs[1, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(total_loss: list, recon_loss: list, kld_loss: list, save_path = False):\n",
    "    plt.figure(figsize = (18,6))\n",
    "    \n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(total_loss, color = \"red\")\n",
    "    plt.title(\"Total Loss vs Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Total Loss\")\n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(recon_loss, color = \"blue\", label = \"recon\")\n",
    "    plt.title(\"Recon Loss vs Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Recon Loss\")\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.plot(kld_loss, color = \"green\", label = \"kld\")\n",
    "    plt.title(\"KLD Loss vs Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"KLD Loss\")\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    else:\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Eval Engine Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_function, train_loader, val_loader, no_epochs, learning_rate, alpha = 1, early_stopping = False, patience = 5, save = True, load = False):\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3)\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    print(f\"Training Model on {DEVICE}\")\n",
    "    \n",
    "    if load:\n",
    "        ckpt_path = ARTIFACT_PATH / \"model_ckpt\" / str(model) / \"model.pt\"\n",
    "        if not ckpt_path.exists():\n",
    "            print(f\"Ckpt_path for {str(model)} does not exist. Training New Model\")\n",
    "        else:\n",
    "            load_model(model, ckpt_path)\n",
    "\n",
    "    best_total = float(\"inf\")\n",
    "    patience_count = 0\n",
    "    history = {\"total_loss\": [], \"recon_loss\": [], \"kld_loss\": []}\n",
    "    \n",
    "    for epoch in range(no_epochs):\n",
    "        model.train()\n",
    "        epoch_loss, recon_epoch_loss, kld_epoch_loss = 0, 0, 0\n",
    "        tk0 = tqdm(train_loader, total=len(train_loader))\n",
    "        for batch_idx, (images, _) in enumerate(tk0):\n",
    "            images = images.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, log_var = model(images)\n",
    "            loss, RECON, KLD = loss_function(recon_batch, images, mu, log_var, alpha = alpha)\n",
    "\n",
    "            loss.backward()\n",
    "            epoch_loss += loss.item()\n",
    "            recon_epoch_loss += RECON.item()\n",
    "            kld_epoch_loss += KLD.item()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "            optimizer.step()\n",
    "            # if (batch_idx + 1) % 100 == 0:\n",
    "            #     print(f\"Train Epoch {epoch + 1} - Total Loss = {loss.item():.4f}, Recon Loss = {RECON.item():.4f}, KLD Loss = {KLD.item():.4f}\")\n",
    "                \n",
    "        print(f\"====> Epoch {epoch + 1} Total Loss = {epoch_loss/len(train_loader):.4f}, Recon Loss = {recon_epoch_loss/len(train_loader):.4f}, KLD Loss =  {kld_epoch_loss/len(train_loader):.4f}\")\n",
    "        \n",
    "        val_total_loss, val_recon_loss, val_kld_loss = eval(model, loss_function, val_loader, epoch, alpha=alpha)\n",
    "        scheduler.step(val_total_loss)\n",
    "        history[\"total_loss\"].append(val_total_loss)\n",
    "        history[\"recon_loss\"].append(val_recon_loss)\n",
    "        history[\"kld_loss\"].append(val_kld_loss)\n",
    "    \n",
    "        if val_total_loss < best_total:\n",
    "            print(f\"{str(model)}: Val Loss improved at epoch {epoch + 1} from {best_total} to {val_total_loss}\")\n",
    "            best_total = val_total_loss\n",
    "            patience_count = 0\n",
    "            if save:\n",
    "                model_path = ARTIFACT_PATH / \"model_ckpt\" / str(model)\n",
    "                if not model_path.exists():\n",
    "                    model_path.mkdir(parents=True)\n",
    "                ckpt_path = str(model_path / \"model.pt\")\n",
    "                save_model(model, ckpt_path)\n",
    "\n",
    "        else:\n",
    "            print(f\"{str(model)}: Validation Accuracy from epoch {epoch + 1} did not improve\")\n",
    "            patience_count += 1\n",
    "            if early_stopping and patience_count == patience:\n",
    "                print(f\"{str(model)}: No val acc improvement for {patience} consecutive epochs. Early Stopped at epoch {epoch + 1}\")\n",
    "        \n",
    "    return history        \n",
    "        \n",
    "def eval(model, loss_function, val_loader, epoch = 0, alpha = 1):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_loss, recon_epoch_loss, kld_epoch_loss = 0, 0, 0\n",
    "        for batch_idx, (images, _) in enumerate(val_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            recon_batch, mu, log_var = model(images)\n",
    "            loss, RECON, KLD = loss_function(recon_batch, images, mu, log_var, alpha=alpha)\n",
    "            epoch_loss += loss.item()\n",
    "            recon_epoch_loss += RECON.item()\n",
    "            kld_epoch_loss += KLD.item()\n",
    "            \n",
    "            if batch_idx == 0:\n",
    "                n = min(images.size(0), 8)\n",
    "                recon_images = recon_batch.view(images.size(0), 1, 28, 28)[:n]\n",
    "                compare_recon(images[:n], recon_images)\n",
    "            \n",
    "    return epoch_loss/len(val_loader), recon_epoch_loss/len(val_loader), kld_epoch_loss/len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Base VAE Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on BCELoss + KL Divergence + Sigmoid + Input Data Normalize between 0 & 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_model_sigmoid = BaseVAE(activation=\"Sigmoid\")\n",
    "loss = BCE_VAE_loss\n",
    "data_manager = MNIST_Dataloader(batch_size = 32, std_normalize=False)\n",
    "train_loader = data_manager.train_loader()\n",
    "test_loader = data_manager.test_loader()\n",
    "\n",
    "history = train(bce_model_sigmoid, loss, train_loader, test_loader, EPOCHS, LEARNING_RATE, early_stopping = False, patience = 5, save = True, load = False, alpha = 1000)\n",
    "plot_loss(history[\"total_loss\"],history[\"recon_loss\"],history[\"kld_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history[\"total_loss\"],history[\"bce_loss\"],history[\"kld_loss\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 20\n",
    "sample_number = 10\n",
    "z = torch.randn(5, latent_size)\n",
    "recon_images = bce_model_sigmoid.decode(z).view(z.size(0), 1, 28, 28)\n",
    "plot_images(recon_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c93027d0fb0f187f1183fadd97ccbf101549c4d550e31bb6d366e03dcf3705f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
